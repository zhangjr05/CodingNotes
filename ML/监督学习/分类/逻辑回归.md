# 逻辑回归 对数几率回归

对数几率回归（Logistic Regression）是一种用于**二分类**的监督学习算法。它的核心思想是：使用Sigmoid函数将线性模型的输出转换为概率值(0到1之间)，用于预测样本属于正类的概率。

---

## 算法步骤

- **模型定义：**
  - 线性部分：$$z = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b$$
  - 概率输出：$$P(y=1 \mid x) = \text{sigmoid}(z) = \frac{1}{1 + e^{-z}}$$

- **损失函数：**
  - 交叉熵损失：$$L(y, p) = -[y \cdot \log(p) + (1 - y) \cdot \log(1 - p)]$$
  - 整体损失：$$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(y^{(i)}, p^{(i)})$$

- **最优化：**
  - 使用梯度下降最小化损失函数
  - 更新参数：$$w = w - \alpha \frac{\partial J}{\partial w} \quad b = b - \alpha \frac{\partial J}{\partial b}$$

---

## 代码实现

```python
import numpy as np

class LogisticRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.w = None
        self.b = 0
    
    def sigmoid(self, z):
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        # 向量化
        X, y = np.asarray(X), np.asarray(y)

        # 初始化参数
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features)
        self.b = 0

        # 梯度下降
        for _ in range(self.n_iterations):
            # 预测概率
            linear_model = np.dot(X, self.w) + self.b
            y_predicted = self.sigmoid(linear_model)

            # 计算交叉熵损失函数的梯度，也是极大似然估计中负对数似然的梯度
            dw = (1 / n_samples) * np.dot(X.T, y_predicted - y)
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # 更新参数
            self.w -= self.learning_rate * dw
            self.b -= self.learning_rate * db
    
    def predict_proba(self, X):
        linear_model = np.dot(X, self.w) + self.b
        return self.sigmoid(linear_model)
    
    def predict(self, X, threshold=0.5):
        return self.predict_proba(X) >= threshold
```

---

## 多分类

```python
class OneVsRestClassifier:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.classifiers = {}
        self.classes = None
    
    def fit(self, X, y):
        X, y = np.asarray(X), np.asarray(y)
        self.classes = np.unique(y)

        # 为每个类别训练一个二分器
        for class_label in self.classes:
            binary_y = (y == class_label).astype(int)
            classifier = LogisticRegression(self.learning_rate, self.n_iterations)
            classifier.fit(X, binary_y)
            self.classifiers[class_label] = classifier
    
    def predict_proba(self, X):
        '''计算所有类别的概率'''
        X = np.asarray(X)
        probabilities = np.zeros((X.shape[0], len(self.classes)))

        for i, class_label in enumerate(self.classes):
            probabilities[:, i] = self.classifiers[class_label].predict_proba(X)
        
        return probabilities
    
    def predict(self, X):
        '''返回最有可能的类别'''
        probabilities = self.predict_proba(X)
        return self.classes[np.argmax(probabilities, axis=1)]
```
