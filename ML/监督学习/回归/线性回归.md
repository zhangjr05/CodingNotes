# 线性回归

线性回归是一种常用的**监督学习**算法，用于预测连续值。核心思想是：**通过拟合一条直线（或高维空间中的超平面）来描述输入特征和目标值之间的关系，并最小化预测值与实际值的误差。**

---

## 算法步骤

- 定义模型形式：$$y = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b$$
- 选择损失函数（通常是均方误差 MSE）
- 通过最小化损失函数找到最优的权重 w 和偏置 b（正规方程 / 梯度下降）
- 使用学习到的参数进行预测

---

## 代码实现

### 正规方程求解（最小二乘法的解析解）

```python
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

class LinearRegression:
    def __init__(self):
        self.w = None
        self.b = None
    
    def fit(self, X, y):
        '''训练线性回归模型'''
        # 添加偏置项
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        # 使用正规方程求解（解析解）
        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
        self.b = theta[0].item()
        self.w = theta[1:].reshape(-1, 1)
        return self

    def predict(self, X):
        '''预测新样本'''
        return X @ self.w + self.b

if __name__ == "__main__":
    X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)
    y = y.reshape(-1, 1)

    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = LinearRegression()
    model.fit(X_train, y_train)
    
    print(f"权重向量 w = {model.w.flatten()}")
    print(f"截距 b = {model.b}")

    # 预测并评估
    y_pred = model.predict(X_test)
    mse = np.mean((y_pred - y_test) ** 2)
    r2 = r2_score(y_test, y_pred)
    
    print(f"均方误差 MSE = {mse:.4f}")
    print(f"决定系数 R² = {r2:.4f}")
```

### 梯度下降算法

```python
class LinearRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.w = None
        self.b = None
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.loss_history = []

    def fit(self, X, y):
        '''使用梯度下降算法训练线性回归模型'''
        X, y = np.asarray(X), np.asarray(y)

        # 初始化参数
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features)
        self.b = 0

        # 梯度下降迭代
        for i in range(self.n_iterations):
            # 计算预测值
            y_pred = self._predict(X)
            
            # 计算误差
            errors = y_pred - y
            
            # 计算梯度
            dw = (1 / n_samples) * (X.T @ errors)
            db = (1 / n_samples) * np.sum(errors)
            
            # 更新参数
            self.w -= self.learning_rate * dw
            self.b -= self.learning_rate * db

            # 计算损失
            loss = (1 / (2 * n_samples)) * np.sum(errors ** 2)
            self.loss_history.append(loss)

        return self
    
    def _predict(self, X):
        '''内部预测函数'''
        return X @ self.w + self.b
    
    def predict(self, X):
        return self._predict(X)
```

### 对数线性回归

```python
class LogLinearRegression:
    def __init__(self, log_y=True, log_x=False):
        self.w = None
        self.b = None
        self.log_y = log_y  # 是否对y取对数
        self.log_x = log_x  # 是否对x取对数
        self.epsilon = 1e-10  # 避免取对数时出现log(0)
    
    def fit(self, X, y):
        '''训练对数线性回归模型'''
        # 对特征和目标应用对数变换
        X_trans = self._transform_X(X)
        y_trans = self._transform_y(y)
        
        # 添加偏置项
        X_b = np.c_[np.ones((X_trans.shape[0], 1)), X_trans]
        
        # 使用正规方程求解
        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y_trans
        self.b = theta[0].item()
        self.w = theta[1:].reshape(-1, 1)
        
        return self
    
    def _transform_X(self, X):
        '''对特征应用对数变换'''
        if self.log_x:
            return np.log(X + self.epsilon)
        return X
    
    def _transform_y(self, y):
        '''对目标变量应用对数变换'''
        if self.log_y:
            return np.log(y + self.epsilon)
        return y
    
    def _inverse_transform_y(self, y_pred):
        '''预测值的反变换'''
        if self.log_y:
            return np.exp(y_pred) - self.epsilon
        return y_pred
    
    def predict(self, X):
        '''预测新样本'''
        X_trans = self._transform_X(X)
        y_pred = X_trans @ self.w + self.b
        return self._inverse_transform_y(y_pred)
```
